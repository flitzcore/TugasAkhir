{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 31\n",
      "Python-dotenv could not parse statement starting at line 31\n",
      "Python-dotenv could not parse statement starting at line 31\n",
      "Python-dotenv could not parse statement starting at line 31\n",
      "Python-dotenv could not parse statement starting at line 31\n",
      "Python-dotenv could not parse statement starting at line 31\n",
      "Python-dotenv could not parse statement starting at line 31\n"
     ]
    }
   ],
   "source": [
    "import langchain_community\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from CustomLoader import CustomDocumentLoader\n",
    "from collections import Counter\n",
    "from pymongo import MongoClient\n",
    "import pysqlite3\n",
    "import sys\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from fastapi import Depends, FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomHuggingFaceEmbeddings(HuggingFaceEmbeddings):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _embed_documents(self, texts):\n",
    "        return super().embed_documents(texts)\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        return self._embed_documents(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chatbot/Chatbot-Tes/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/chatbot/Chatbot-Tes/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "mongo_uri = \"mongodb+srv://tes-dev:8qsnPptZZapl0ipI@clusterdev.c7whc.mongodb.net/?retryWrites=true&w=majority&appName=ClusterDev\"\n",
    "db_name = \"test\"\n",
    "collection_name = \"datas\"  # Replace with your actual collection name\n",
    "\n",
    "        # Initialize the custom document loader\n",
    "loader = CustomDocumentLoader(mongo_uri, db_name, collection_name)\n",
    "\n",
    "client = chromadb.Client(Settings( persist_directory=\"db/\"))\n",
    "try:\n",
    "    collection = client.create_collection(name=\"SOP_collection\", embedding_function=CustomHuggingFaceEmbeddings(\n",
    "        model_name=\"Alibaba-NLP/gte-multilingual-base\",\n",
    "        model_kwargs = {'trust_remote_code': True}\n",
    "    ))\n",
    "except:\n",
    "    collection = client.get_collection(name=\"SOP_collection\", embedding_function=CustomHuggingFaceEmbeddings(\n",
    "        model_name=\"Alibaba-NLP/gte-multilingual-base\",\n",
    "        model_kwargs = {'trust_remote_code': True}\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    base_url='http://127.0.0.1:11434',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "def updateData():\n",
    "    client.delete_collection(\"SOP_collection\")  # This will delete all items from the collection\n",
    "    collection = client.create_collection(name=\"SOP_collection\", embedding_function=CustomHuggingFaceEmbeddings(\n",
    "        model_name=\"Alibaba-NLP/gte-multilingual-base\",\n",
    "        model_kwargs = {'trust_remote_code': True}\n",
    "    ))\n",
    "\n",
    "#     loader = CustomDocumentLoader(\n",
    "#     file_path=\"./dataSop.json\", \n",
    "#    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    # print(documents)\n",
    "    listOfDocuments=[]\n",
    "    listOfMetadatas=[]\n",
    "    listOfIds=[]\n",
    "    for doc  in documents:\n",
    "        listOfDocuments.append(doc.page_content)\n",
    "        listOfMetadatas.append({\"source\":doc.metadata['type']})\n",
    "        listOfIds.append(doc.metadata['type']+ \"_\" + str(uuid.uuid4()))\n",
    "    # print(listOfIds)\n",
    "    # print(listOfMetadatas)\n",
    "    # print(listOfDocuments)\n",
    "    collection.add(\n",
    "        documents=listOfDocuments,\n",
    "        metadatas=listOfMetadatas,\n",
    "        ids=listOfIds\n",
    "    )\n",
    "def query(question):\n",
    "    result=collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=4\n",
    "    )\n",
    "    return result\n",
    "def findCommonType(question):\n",
    "    results = query(question)\n",
    "    metadatas = results['metadatas']\n",
    "    distances = results['distances']\n",
    "\n",
    "    # Initialize an empty list to store the filtered sources\n",
    "    filtered_sources = []\n",
    "\n",
    "    # Iterate over both metadatas and distances simultaneously\n",
    "    for meta_list, dist_list in zip(metadatas, distances):\n",
    "        for meta, distance in zip(meta_list, dist_list):\n",
    "            # Only consider sources where distance is less than 1\n",
    "            if distance < 1.5:\n",
    "                filtered_sources.append(meta['source'])\n",
    "\n",
    "    # Count the occurrences of each source in the filtered list\n",
    "    source_count = Counter(filtered_sources)\n",
    "\n",
    "    # Find the most common source\n",
    "    most_common_source = source_count.most_common(1)\n",
    "    \n",
    "        # If there is a common source, return it and its count\n",
    "    if most_common_source:\n",
    "        return most_common_source[0]  # returns (source, count)\n",
    "    else:\n",
    "        # If no source meets the condition, return None or (None, 0)\n",
    "        return (None, 0)\n",
    "def respondQuestion(question,history):\n",
    "    commonTypes,commonTypesCount=findCommonType(question)\n",
    "    # print(\"commontype: \"+ str(commonTypes)+\" count: \"+str(commonTypesCount))\n",
    "    queryString=''\n",
    "    if(commonTypesCount>=1):\n",
    "        queryString=loader.load_string_from_types(commonTypes)\n",
    "    llmQuery = f\"\"\"GENERAL INSTRUCTIONS\n",
    "    You are reza an Indonesian assistant for question-answering tasks. You always answer in Bahasa Indonesia. You don't always know the answer, so if you don't know the answer, just say that you don't know. Below is what you know, you don't need to use all of it to answer:\n",
    "    {queryString}\n",
    "\n",
    "    PREVIOUS INTERACTION\n",
    "    {history}\n",
    "    \n",
    "    USER QUESTION\n",
    "    {question}\n",
    "\n",
    "    ANSWER \n",
    "    \"\"\"\n",
    "\n",
    "    # llmQuery = f'''\n",
    "    # {question}\n",
    "    # You are Reza an assistant for question-answering tasks. \n",
    "    # Use the following pieces of retrieved context to answer the question. \n",
    "    # If you don't know the answer, just say that you don't know. Always answer in Bahasa Indonesia\n",
    "    # Here is chat history between you and a user.\n",
    "    # Chat History:{history}\n",
    "    # Here is the question and context needed to answer the question\n",
    "    # Question: {question} \n",
    "    # Context: {queryString}\n",
    " \n",
    "    # Answer:\n",
    "    # '''\n",
    "    return llm.invoke(llmQuery).content\n",
    "updateData()\n",
    "# print(respondQuestion(\"bagaimana melakukan absen elektronik?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Halo! Saya siap membantu menjawab pertanyaan Anda. Apa yang ingin Anda tanyakan?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respondQuestion(\"hi\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1680808]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [1680808]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Allows all origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allows all methods\n",
    "    allow_headers=[\"*\"],  # Allows all headers\n",
    ")\n",
    "\n",
    "class PromptRequest(BaseModel):\n",
    "    text: str\n",
    "    history: str\n",
    "class PromptResponse(BaseModel):\n",
    "    text: str\n",
    "class UpdateResponse(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/predict\", response_model=PromptResponse)\n",
    "def predict(request: PromptRequest):\n",
    "    question = request.text\n",
    "    history=request.history\n",
    "    result= respondQuestion(question,history)\n",
    "\n",
    "    return PromptResponse(\n",
    "        text= result,\n",
    "    )\n",
    "@app.post(\"/update\", response_model=UpdateResponse)\n",
    "def update():\n",
    "    updateData()\n",
    "    return UpdateResponse(\n",
    "        text= \"Success\",\n",
    "    )\n",
    "\n",
    "listener = ngrok.forward(\n",
    "    # session configuration\n",
    "    addr=\"10.3.142.13:8000\",\n",
    "    authtoken=\"2hoMzwSwKafINnhGpRXZqICd0OW_4z7FxD7VNAjhhxfD7cUhM\",\n",
    "    domain=\"glorious-easily-mammoth.ngrok-free.app\")\n",
    "# print('Public URL:', listener.url())\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000, host=\"0.0.0.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
